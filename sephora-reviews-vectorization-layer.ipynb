{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41c4b449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a3f800a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review_text</th>\n",
       "      <th>label</th>\n",
       "      <th>rating</th>\n",
       "      <th>text_word_count</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>tokenized_text_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Gel type moisturizers aren’t typically my go-t...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>51</td>\n",
       "      <td>gel type moisturizers aren’t typically my goto...</td>\n",
       "      <td>['gel', 'type', 'moistur', '’', 'typic', 'goto...</td>\n",
       "      <td>gel type moistur ’ typic goto one impress gel ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>smells great! works really well on my acne sca...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>44</td>\n",
       "      <td>smells great works really well on my acne scar...</td>\n",
       "      <td>['smell', 'great', 'work', 'realli', 'well', '...</td>\n",
       "      <td>smell great work realli well acn scare need co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Caused me so many breakouts. I thought this wo...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>caused me so many breakouts i thought this wou...</td>\n",
       "      <td>['caus', 'mani', 'breakout', 'thought', 'would...</td>\n",
       "      <td>caus mani breakout thought would good use clea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>I could not be happier with this mask! I alway...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>45</td>\n",
       "      <td>i could not be happier with this mask i always...</td>\n",
       "      <td>['could', 'happier', 'mask', 'alway', 'use', '...</td>\n",
       "      <td>could happier mask alway use overnight mask am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Fastest acting spot reducer I have ever tried....</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>fastest acting spot reducer i have ever tried ...</td>\n",
       "      <td>['fastest', 'act', 'spot', 'reduc', 'ever', 't...</td>\n",
       "      <td>fastest act spot reduc ever tri got watch one ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                        review_text  label  \\\n",
       "0           0  Gel type moisturizers aren’t typically my go-t...    1.0   \n",
       "1           1  smells great! works really well on my acne sca...    1.0   \n",
       "2           2  Caused me so many breakouts. I thought this wo...    0.0   \n",
       "3           3  I could not be happier with this mask! I alway...    1.0   \n",
       "4           4  Fastest acting spot reducer I have ever tried....    1.0   \n",
       "\n",
       "   rating  text_word_count                                         clean_text  \\\n",
       "0       5               51  gel type moisturizers aren’t typically my goto...   \n",
       "1       4               44  smells great works really well on my acne scar...   \n",
       "2       1               29  caused me so many breakouts i thought this wou...   \n",
       "3       5               45  i could not be happier with this mask i always...   \n",
       "4       5               50  fastest acting spot reducer i have ever tried ...   \n",
       "\n",
       "                                      tokenized_text  \\\n",
       "0  ['gel', 'type', 'moistur', '’', 'typic', 'goto...   \n",
       "1  ['smell', 'great', 'work', 'realli', 'well', '...   \n",
       "2  ['caus', 'mani', 'breakout', 'thought', 'would...   \n",
       "3  ['could', 'happier', 'mask', 'alway', 'use', '...   \n",
       "4  ['fastest', 'act', 'spot', 'reduc', 'ever', 't...   \n",
       "\n",
       "                                    tokenized_text_2  \n",
       "0  gel type moistur ’ typic goto one impress gel ...  \n",
       "1  smell great work realli well acn scare need co...  \n",
       "2  caus mani breakout thought would good use clea...  \n",
       "3  could happier mask alway use overnight mask am...  \n",
       "4  fastest act spot reduc ever tri got watch one ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_sample= pd.read_csv(r'C:\\Users\\shanm\\OneDrive\\Desktop\\Git_data\\Sephora_sample\\X_reviews_sample.csv',low_memory =True)\n",
    "reviews_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "516f071b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21278    Shiseido has a great reputation with putting o...\n",
      "Name: review_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "reviews_sample.loc[reviews_sample['text_word_count']==409]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3bec3524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    156820.000000\n",
       "mean         58.845606\n",
       "std          41.760496\n",
       "min           1.000000\n",
       "25%          31.000000\n",
       "50%          49.000000\n",
       "75%          75.000000\n",
       "max         409.000000\n",
       "Name: text_word_count, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_sample['text_word_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cec359d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= reviews_sample['clean_text']\n",
    "y = reviews_sample['label'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9fcf9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c32ba47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,LSTM, TextVectorization\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "692f0e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000  # Maximum vocab size.\n",
    "max_len = 200  # Sequence length to pad the outputs to.\n",
    "\n",
    "# Create the layer.\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    " max_tokens=max_features,\n",
    " output_mode='int',\n",
    " output_sequence_length=max_len)\n",
    "\n",
    "# Now that the vocab layer has been created, call `adapt` on the\n",
    "# text-only dataset to create the vocabulary. You don't have to batch,\n",
    "# but for large datasets this means we're not keeping spare copies of\n",
    "# the dataset.\n",
    "vectorize_layer.adapt(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac75709f",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xc3 in position 0: unexpected end of data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28140/978233260.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvectorize_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\keras_env\\lib\\site-packages\\keras\\layers\\preprocessing\\text_vectorization.py\u001b[0m in \u001b[0;36mget_vocabulary\u001b[1;34m(self, include_special_tokens)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[0mvocabulary\u001b[0m \u001b[0mwill\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0minclude\u001b[0m \u001b[0many\u001b[0m \u001b[0mpadding\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mOOV\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m     \"\"\"\n\u001b[1;32m--> 382\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lookup_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minclude_special_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mvocabulary_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\keras_env\\lib\\site-packages\\keras\\layers\\preprocessing\\index_lookup.py\u001b[0m in \u001b[0;36mget_vocabulary\u001b[1;34m(self, include_special_tokens)\u001b[0m\n\u001b[0;32m    321\u001b[0m       \u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlookup_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m       \u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minvert\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 323\u001b[1;33m       \u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tensor_vocab_to_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    324\u001b[0m     lookup = collections.defaultdict(lambda: self.oov_token,\n\u001b[0;32m    325\u001b[0m                                      zip(indices, vocab))\n",
      "\u001b[1;32m~\\anaconda3\\envs\\keras_env\\lib\\site-packages\\keras\\layers\\preprocessing\\string_lookup.py\u001b[0m in \u001b[0;36m_tensor_vocab_to_numpy\u001b[1;34m(self, vocabulary)\u001b[0m\n\u001b[0;32m    348\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_tensor_vocab_to_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m     \u001b[0mvocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\keras_env\\lib\\site-packages\\keras\\layers\\preprocessing\\string_lookup.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    348\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_tensor_vocab_to_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m     \u001b[0mvocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\util\\compat.py\u001b[0m in \u001b[0;36mas_text\u001b[1;34m(bytes_or_text, encoding)\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mbytes_or_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbytes_or_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mbytes_or_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Expected binary or unicode string, got %r'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbytes_or_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xc3 in position 0: unexpected end of data"
     ]
    }
   ],
   "source": [
    "vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f02ee122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(200,), dtype=int64, numpy=\n",
       "array([1371,   45,    8,   56, 3836,   23,  455,   35,  654,   15,    2,\n",
       "         22,  457,    9,   94,    7,   15,   35,   11,  194,   34, 1526,\n",
       "          7,   48,  223,    2, 1135,    5, 1055,    4,  373,    3,   60,\n",
       "          8,   79, 2032,    9,  554,   23,    3,    2, 2364,   18, 3802,\n",
       "          7,    2,   24,   20,   32,   53,   19,    9,  285, 3483,   55,\n",
       "        832,  143,    3,  182,    5,   88,   12,  214,   13, 1836,    4,\n",
       "         36,  790,  268, 2729,   82,  206,   25,    8, 2794,   20,    2,\n",
       "         22,  393,   21,    5,  130,   22,   16,  997,   43,   38,    3,\n",
       "         22,   36,  587,   34, 1022,    6,   27,  544,  847,    2,  103,\n",
       "        176,    9,   75,   29,    7,   15,  234,    6,  160,   11,    8,\n",
       "        325,    2,   50,   29,   83,   97,  212,  234,    5,  160,   16,\n",
       "         87,  132,   20,    2,  213,   74,    8,   62,  446,   14,  106,\n",
       "          3,   22,  559,   11,   25,    5,   44,   14,    7,   12,   20,\n",
       "        533,    9,    5,  841,    4,   26,   16,  924,  200,   26, 1259,\n",
       "          8,   96, 2036,    9,    3,  426,    3,  553,  143,    5,   10,\n",
       "        356,   76,   34, 1022,    6,   27,    4,  455,  400,   17,    5,\n",
       "         27,    2,  761,    6,  451,  148,    4,  169,    5,   66,   17,\n",
       "          5, 1247,  150,    5,  146,    4,   67,  234,    5, 2245,  323,\n",
       "        367,  136], dtype=int64)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_layer(X[21278 ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
